{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/444112029012/4112029012_cal/blob/main/data_collection/html_dataset/%E5%89%B5%E5%BB%BA%E9%9D%9EAI%E6%AC%84%E4%BD%8DHTML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q-pEw7-OE1pB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Colab é€²è¡Œmatplotlibç¹ªåœ–æ™‚é¡¯ç¤ºç¹é«”ä¸­æ–‡\n",
        "# ä¸‹è¼‰å°åŒ—æ€æºé»‘é«”ä¸¦å‘½åtaipei_sans_tc_beta.ttfï¼Œç§»è‡³æŒ‡å®šè·¯å¾‘\n",
        "!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "# æ”¹styleè¦åœ¨æ”¹fontä¹‹å‰\n",
        "# plt.style.use('seaborn')\n",
        "\n",
        "matplotlib.font_manager.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')\n",
        "matplotlib.rc('font', family='Taipei Sans TC Beta')\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "def load_data() -> pd.DataFrame:\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "  # å°‡ä¸Šå‚³çš„æª”æ¡ˆè®€å–åˆ° pandas DataFrame ä¸­\n",
        "  # å‡è¨­ä¸Šå‚³çš„æª”æ¡ˆæ˜¯ CSV æ ¼å¼ã€‚å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œæ‚¨å¯èƒ½éœ€è¦èª¿æ•´è®€å–å‡½æ•¸ï¼ˆä¾‹å¦‚ï¼špd.read_excelï¼‰\n",
        "  file_name = next(iter(uploaded))\n",
        "  df = pd.read_csv(file_name)\n",
        "\n",
        "  # é¡¯ç¤º DataFrame çš„å‰ 5 è¡Œ\n",
        "  display(df.head())\n",
        "  return df\n",
        "# df = load_data()\n",
        "def save_df(df, filename = 'phishing_dataset_for_trainning'):\n",
        "    \"\"\"\n",
        "    å°‡æŒ‡å®šçš„ DataFrame å­˜æˆ CSV æª”æ¡ˆã€‚\n",
        "\n",
        "    åƒæ•¸:\n",
        "    - df: è¦å„²å­˜çš„ DataFrame\n",
        "    - filename: å­˜æª”çš„æª”åï¼ˆä¾‹å¦‚ 'output.csv'ï¼‰\n",
        "\n",
        "    åŠŸèƒ½:\n",
        "    - ä½¿ç”¨ UTF-8 with BOM ç·¨ç¢¼é¿å…ä¸­æ–‡äº‚ç¢¼\n",
        "    - ä¸åŒ…å«ç´¢å¼•æ¬„ä½\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"âœ… è³‡æ–™å·²æˆåŠŸå„²å­˜è‡³ '{filename}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å„²å­˜å¤±æ•—ï¼š{e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_geNmBHqsc5e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# !pip install google-generativeai\n",
        "!pip install selenium\n",
        "# !pip install webdriver-manager\n",
        "!apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "# 2. æ‰‹å‹•å®‰è£èˆ‡æœ€æ–° WebDriver ç›¸ç¬¦çš„ Chrome ç€è¦½å™¨\n",
        "#    é€™èƒ½ç¢ºä¿ç‰ˆæœ¬åŒ¹é…ï¼Œé¿å…å•Ÿå‹•éŒ¯èª¤\n",
        "# !wget -q https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/114.0.5735.90/linux64/chrome-linux64.zip\n",
        "# !unzip -q chrome-linux64.zip -d /bin/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install google-colab-selenium"
      ],
      "metadata": {
        "id": "TIou8YmMYAJn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ4Jmrf8FOnk",
        "outputId": "8ff6a2c5-a938-4337-d8fa-f05270435f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… è³‡æ–™å·²æˆåŠŸå„²å­˜è‡³ 'phishing_dataset_expansion_forEmbeddingModule_24000to36000.csv'\n"
          ]
        }
      ],
      "source": [
        "save_df(df_with_html_features, 'phishing_dataset_expansion_1_html_test.csv') # è¨˜å¾—!!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "import ipaddress\n",
        "import signal\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Optional\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# ====== å¯é¸: å¦‚æœéœ€è¦è™•ç†å‹•æ…‹å…§å®¹ï¼Œè«‹å–æ¶ˆè¨»é‡‹ä»¥ä¸‹å…§å®¹ä¸¦å®‰è£ç›¸é—œåº« ======\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "# from webdriver_manager.chrome import ChromeDriverManager\n",
        "from google_colab_selenium import Chrome\n",
        "# =====================================================================\n",
        "\n",
        "\n",
        "def get_html_content(url, timeout=20, max_retries=2):\n",
        "    \"\"\"\n",
        "    æ ¹æ“š URL ç²å–ç¶²é çš„å®Œæ•´ HTML å…§å®¹ï¼Œä¸¦åµæ¸¬æ˜¯å¦æœ‰è½‰å‘ã€‚\n",
        "    å…ˆä½¿ç”¨ requests çˆ¬å–ï¼Œå¦‚æœå…§å®¹ç‚ºç©ºæˆ–å¤±æ•—å‰‡ä½¿ç”¨ Seleniumã€‚\n",
        "    Args:\n",
        "        url (str): ç›®æ¨™ç¶²é çš„ URLã€‚\n",
        "        timeout (int): è«‹æ±‚è¶…æ™‚æ™‚é–“ (ç§’)ã€‚\n",
        "        max_retries (int): å¤±æ•—æ™‚é‡è©¦æ¬¡æ•¸ã€‚\n",
        "    Returns:\n",
        "        tuple[str, bool]: ç¶²é çš„ HTML å…§å®¹ï¼Œå¦‚æœç²å–å¤±æ•—å‰‡è¿”å› Noneã€‚\n",
        "                           ä»¥åŠä¸€å€‹å¸ƒæ—å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ç™¼ç”Ÿäº†è½‰å‘ã€‚\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.66 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=timeout, verify=False)\n",
        "            response.raise_for_status()\n",
        "            html_content = response.text\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            body_content = soup.find('body')\n",
        "            response.close()\n",
        "            # æª¢æŸ¥å…§å®¹æ˜¯å¦ç‚ºç©ºæˆ–éçŸ­ï¼ˆå¯èƒ½æ˜¯å‹•æ…‹å…§å®¹ï¼‰\n",
        "            if html_content and len(html_content.strip()) > 100:\n",
        "                # æª¢æŸ¥é é¢å…§å®¹æ˜¯å¦åŒ…å« \"page not found\" ç›¸é—œé—œéµè©\n",
        "                not_found_keywords = ['page not found', 'error 404', 'page does not exist', 'æ‰¾ä¸åˆ°é é¢', 'é é¢ä¸å­˜åœ¨']\n",
        "                matched_keywords = 0\n",
        "                try:\n",
        "                  body_text = body_content.get_text(strip=True)\n",
        "                  body_lower = body_text.lower()\n",
        "\n",
        "                  # è¨ˆç®—åŒ¹é…çš„é—œéµè©æ•¸é‡\n",
        "                  matched_keywords = sum(1 for keyword in not_found_keywords if keyword in body_lower)\n",
        "                except:\n",
        "                  html_lower = html_content.lower()\n",
        "\n",
        "                  # è¨ˆç®—åŒ¹é…çš„é—œéµè©æ•¸é‡\n",
        "                  matched_keywords = sum(1 for keyword in not_found_keywords if keyword in html_lower)\n",
        "                if matched_keywords >= 2:\n",
        "                    print(f\"âŒ URL {url} é é¢å…§å®¹åŒ…å« {matched_keywords} å€‹ 'page not found' ç›¸é—œé—œéµè©ï¼Œç›´æ¥è·³é\")\n",
        "                    return None, False\n",
        "\n",
        "                print(f\"âœ… ä½¿ç”¨ requests æˆåŠŸç²å– {url} çš„å…§å®¹\")\n",
        "                return html_content, False\n",
        "            else:\n",
        "                print(f\"âš ï¸  requests ç²å–çš„å…§å®¹ç‚ºç©ºæˆ–éçŸ­ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium\")\n",
        "                break\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            # æª¢æŸ¥æ˜¯å¦ç‚º 404 æˆ–å…¶ä»– \"page not found\" ç›¸é—œéŒ¯èª¤\n",
        "            if response.status_code == 404:\n",
        "                print(f\"âŒ URL {url} è¿”å› 404 éŒ¯èª¤ï¼Œç›´æ¥è·³é\")\n",
        "                return None, False\n",
        "            elif response.status_code >= 400:\n",
        "                print(f\"âŒ URL {url} è¿”å› HTTP {response.status_code} éŒ¯èª¤ï¼Œç›´æ¥è·³é\")\n",
        "                return None, False\n",
        "            else:\n",
        "                print(f\"[é‡è©¦ {attempt+1}/{max_retries}] Requests ç²å– {url} æ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {e}\")\n",
        "                if attempt == max_retries:\n",
        "                    print(f\"âŒ Requests é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium\")\n",
        "                    break\n",
        "                time.sleep(0.5)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[é‡è©¦ {attempt+1}/{max_retries}] Requests ç²å– {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "            if attempt == max_retries:\n",
        "                print(f\"âŒ Requests é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium\")\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def set_driver():\n",
        "    print('é–‹å§‹è¨­å®šDRIVER...')\n",
        "    try:\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless') # å•Ÿç”¨ç„¡é ­æ¨¡å¼ï¼Œè®“ç€è¦½å™¨åœ¨èƒŒæ™¯é‹è¡Œï¼Œä¸é¡¯ç¤ºè¦–çª—\n",
        "        chrome_options.add_argument('--no-sandbox') # é¿å…åœ¨ Colab ç’°å¢ƒä¸‹å¯èƒ½ç™¼ç”Ÿçš„æ¬Šé™å•é¡Œ\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage') # è§£æ±º /dev/shm åˆ†å€ç©ºé–“ä¸è¶³çš„å•é¡Œï¼Œé€™åœ¨ Docker æˆ– Colab ä¸­å¾ˆå¸¸è¦‹\n",
        "        chrome_options.add_argument('--disable-gpu') # ç¦ç”¨ GPU åŠ é€Ÿ\n",
        "        chrome_options.add_argument('--disable-setuid-sandbox') # è®“ Chrome å¯ä»¥åœ¨ä¸å®‰å…¨çš„ç’°å¢ƒä¸‹åŸ·è¡Œ\n",
        "\n",
        "        # æ¨¡æ“¬ä¸€å€‹çœŸå¯¦çš„ User-Agentï¼Œé¿å…è¢«ç¶²ç«™åµæ¸¬ç‚ºçˆ¬èŸ²\n",
        "        chrome_options.add_argument('--user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"')\n",
        "        prefs = {\n",
        "        \"download.default_directory\": \"NUL\",      # (å¦‚æœæ˜¯ Windows, ç”¨é€™å€‹)\n",
        "        \"download.prompt_for_download\": False,      # ä¸è©¢å•ä¸‹è¼‰ä½ç½®\n",
        "        \"download.directory_upgrade\": True,\n",
        "        \"profile.default_content_settings.popups\": 0, # å°é–å½ˆå‡ºè¦–çª—\n",
        "        \"safebrowsing.enabled\": False,                # é—œé–‰å®‰å…¨ç€è¦½ (é¿å…å®ƒå¹²æ“¾çˆ¬èŸ²)\n",
        "        \"profile.default_content_setting_values.automatic_downloads\": 2 # *** é—œéµï¼šç¦æ­¢è‡ªå‹•ä¸‹è¼‰ ***\n",
        "        }\n",
        "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "        driver = Chrome(options=chrome_options)\n",
        "        driver.set_page_load_timeout(50)\n",
        "        driver.set_script_timeout(50)\n",
        "        print(\"WebDriver åˆå§‹åŒ–å®Œæˆã€‚\")\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print('driverè¨­å®šéŒ¯èª¤:', e)\n",
        "\n",
        "def _fetch_dynamic_content(driver, url: str) -> Tuple[Optional[str], str]:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ Selenium çˆ¬å–å‹•æ…‹ç¶²é å…§å®¹ã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "\n",
        "        driver.get(url)\n",
        "        # ç­‰å¾…ç¶²é å…§å®¹è¼‰å…¥ï¼Œå¯ä¾æ“šç¶²ç«™ç‰¹æ€§èª¿æ•´æ™‚é–“\n",
        "        time.sleep(3.5)\n",
        "        for i in range(1, 3):\n",
        "            driver.execute_script(f\"window.scrollTo(0, document.body.scrollHeight * {i/3});\")\n",
        "            time.sleep(0.8)\n",
        "\n",
        "        print(f\"âœ… ç¶²é  {url} å‹•æ…‹çˆ¬å–å®Œæˆ...\")\n",
        "        text = driver.page_source\n",
        "        return (text, 'OK_Dynamic') if text else (None, 'OK_Dynamic_Empty')\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ éŒ¯èª¤: ä½¿ç”¨ Selenium çˆ¬å–ç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è¨Šæ¯: {e}\")\n",
        "        return None, 'Error_Selenium'\n",
        "\n",
        "\n",
        "def extract_html_features(df: pd.DataFrame, url_column: str = 'url') -> pd.DataFrame:\n",
        "\n",
        "    if url_column not in df.columns:\n",
        "        raise ValueError(f\"DataFrame ä¸­æœªæ‰¾åˆ°æŒ‡å®šçš„ URL æ¬„ä½: '{url_column}'\")\n",
        "\n",
        "    # ç¢ºä¿æ‰€æœ‰ URL éƒ½æ˜¯å­—ä¸²é¡å‹ï¼Œä¸¦è™•ç†å¯èƒ½çš„ NaN\n",
        "    df[url_column] = df[url_column].astype(str).replace('nan', '')\n",
        "\n",
        "    # æ›´æ–°ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ŒåŠ å…¥å…©å€‹è½‰å‘ç‰¹å¾µ\n",
        "    html_feature_columns = [\n",
        "        'phish_hints', 'domain_in_brand', 'nb_hyperlinks', 'ratio_intHyperlinks',\n",
        "        'ratio_extHyperlinks', 'ratio_extRedirection', 'ratio_extErrors',\n",
        "        'external_favicon', 'links_in_tags', 'ratio_extMedia', 'safe_anchor',\n",
        "        'empty_title', 'domain_in_title', 'domain_with_copyright',\n",
        "        'has_meta_refresh', 'has_js_redirect',\n",
        "        'feature_extracted'  # æ–°å¢æ¬„ä½\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    driver = set_driver()\n",
        "    RECYCLE_DRIVER_COUNT = 100\n",
        "    if 'feature_extracted' not in df.columns:\n",
        "        df[[html_feature_columns]] = np.nan\n",
        "    try:\n",
        "        for index, row in df.iterrows():\n",
        "            if len(results) % RECYCLE_DRIVER_COUNT == 0 and results:\n",
        "                driver.quit()\n",
        "                driver = set_driver()\n",
        "            url = row[url_column]\n",
        "            if row['feature_extracted'] == 0 or row['feature_extracted'] == 1:\n",
        "                continue\n",
        "            if not url:\n",
        "                df.at[index, 'feature_extracted'] = 0.0\n",
        "                continue\n",
        "            # if row['feature_extracted'] == 1:\n",
        "            #     for col in html_feature_columns:\n",
        "            #         features[col] = row[col]\n",
        "            #     results.append(features)\n",
        "            #     continue\n",
        "            for col in html_feature_columns:\n",
        "                df.at[index, col] = 0.0\n",
        "            print(f\"æ­£åœ¨è™•ç†ç¬¬ {index+1} / {len(df)} ç­† URL: {url}\")\n",
        "\n",
        "            # è¨­å®šå–®å€‹ URL çš„æœ€å¤§è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰\n",
        "            max_url_time = 30\n",
        "\n",
        "            try:\n",
        "                # --- é—œéµä¿®æ”¹ï¼šå‘¼å« get_html_content ä¸¦æ¥æ”¶å…©å€‹è¿”å›å€¼ ---\n",
        "                html_content, dynamic_content = safe_process_url(driver, url, max_time=120)\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ è™•ç† URL {url} æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\")\n",
        "                df.at[index, 'feature_extracted'] = 0.0\n",
        "                continue\n",
        "\n",
        "            if html_content and dynamic_content:\n",
        "                try:\n",
        "                    soup_static = BeautifulSoup(html_content, 'html.parser')\n",
        "                    parsed_url = urlparse(url)\n",
        "                    base_domain = parsed_url.netloc.split(':')[0]\n",
        "                    if base_domain.startswith('www.'):\n",
        "                        base_domain = base_domain[4:]\n",
        "                    redirect_keywords = [\n",
        "                        \"window.location.href\",\n",
        "                        \"location.href\",\n",
        "                        \"location.assign\",\n",
        "                        \"location.replace\",\n",
        "                        \"window.navigate\"\n",
        "                    ]\n",
        "                    # --- æ–°å¢çš„åµæ¸¬é‚è¼¯ ---\n",
        "                    # 15. åµæ¸¬ meta è½‰å‘\n",
        "                    meta_refresh_tag = soup_static.find('meta', attrs={'http-equiv': lambda x: x and x.lower() == 'refresh'})\n",
        "                    if meta_refresh_tag:\n",
        "                        content = meta_refresh_tag.get(\"content\", \"\")\n",
        "                        df.at[index, 'has_meta_refresh'] = 1.0 if \"url=\" in content.lower() else 0.0  # ç¢ºèªæ˜¯ redirectï¼Œä¸æ˜¯å–®ç´” reload\n",
        "                    else:\n",
        "                        df.at[index, 'has_meta_refresh'] = 0.0\n",
        "\n",
        "                    # 16. åµæ¸¬ JavaScript è½‰å‘ (ä¾†è‡ª get_html_content çš„è¿”å›å€¼)\n",
        "                    df.at[index, 'has_js_redirect'] = 1.0 if soup_static.find(\"script\", string=lambda s: any(k in s for k in redirect_keywords) if s else False) else 0.0\n",
        "                    # ----------------------\n",
        "                    soup_dynamic = BeautifulSoup(dynamic_content, 'html.parser')\n",
        "\n",
        "                    # --- 1. phish_hints: HTML å…§å®¹ä¸­æ˜¯å¦å­˜åœ¨å¸¸è¦‹çš„é‡£é­šæç¤ºè©èª ---\n",
        "                    phish_keywords = ['login', 'signin', 'account update', 'verify account',\n",
        "                                    'security alert', 'password', 'bank', 'paypal', 'credit card',\n",
        "                                    'ç·Šæ€¥', 'é©—è­‰', 'ç™»å…¥', 'å¸³æˆ¶æ›´æ–°', 'å®‰å…¨è­¦å‘Š', 'å¯†ç¢¼']\n",
        "                    text_content = soup_dynamic.get_text().lower()\n",
        "                    df.at[index, 'phish_hints'] = 1 if any(kw in text_content for kw in phish_keywords) else 0.0\n",
        "\n",
        "                    # --- 2. domain_in_brand: ç¶²ç«™å…§å®¹ä¸­æåŠçš„å“ç‰Œåç¨±æ˜¯å¦èˆ‡åŸŸåä¸€è‡´ ---\n",
        "                    # ç°¡åŒ–è™•ç†: æª¢æŸ¥åŸŸåæ ¸å¿ƒéƒ¨åˆ†æ˜¯å¦å‡ºç¾åœ¨ meta description, title æˆ– copyright ä¸­\n",
        "                    # æ›´ç²¾ç¢ºéœ€è¦å“ç‰Œåç¨±åˆ—è¡¨æˆ– NLP å¯¦é«”è­˜åˆ¥\n",
        "                    brand_match = 0\n",
        "                    domain_parts = base_domain.split('.')\n",
        "                    # æ”¹é€²åŸŸåæå–é‚è¼¯ï¼šè™•ç†å¤šç´šåŸŸå\n",
        "                    if len(domain_parts) >= 2:\n",
        "                        # å°æ–¼å¸¸è¦‹çš„é ‚ç´šåŸŸåï¼Œå–å€’æ•¸ç¬¬äºŒå€‹éƒ¨åˆ†\n",
        "                        tld = domain_parts[-1]\n",
        "                        if tld in ['com', 'org', 'net', 'edu', 'gov', 'mil']:\n",
        "                            core_domain = domain_parts[-2]\n",
        "                        else:\n",
        "                            # å°æ–¼å…¶ä»–åŸŸåï¼Œå–å€’æ•¸ç¬¬ä¸‰å€‹éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
        "                            core_domain = domain_parts[-3] if len(domain_parts) >= 3 else domain_parts[-2]\n",
        "                    else:\n",
        "                        core_domain = domain_parts[0]\n",
        "\n",
        "                    title_tag = soup_dynamic.find('title')\n",
        "                    if title_tag and core_domain in title_tag.get_text().lower():\n",
        "                        brand_match = 1\n",
        "                    elif soup_dynamic.find('meta', attrs={'name': 'description'}) and core_domain in soup_dynamic.find('meta', attrs={'name': 'description'})['content'].lower():\n",
        "                        brand_match = 1\n",
        "                    df.at[index, 'domain_in_brand'] = brand_match\n",
        "\n",
        "\n",
        "                    # --- 3. nb_hyperlinks: ç¶²é ä¸­è¶…é€£çµçš„ç¸½æ•¸ ---\n",
        "                    all_links = soup_dynamic.find_all('a', href=True)\n",
        "                    df.at[index, 'nb_hyperlinks'] = len(all_links)\n",
        "\n",
        "                    # --- 4. ratio_intHyperlinks: å…§éƒ¨è¶…é€£çµçš„æ¯”ä¾‹ ---\n",
        "                    # --- 5. ratio_extHyperlinks: å¤–éƒ¨è¶…é€£çµçš„æ¯”ä¾‹ ---\n",
        "                    internal_links = 0\n",
        "                    external_links = 0\n",
        "                    for link_tag in all_links:\n",
        "                        href = link_tag['href']\n",
        "                        full_url = urljoin(url, href)\n",
        "                        linked_domain = urlparse(full_url).netloc\n",
        "                        if linked_domain == parsed_url.netloc:\n",
        "                            internal_links += 1\n",
        "                        else:\n",
        "                            external_links += 1\n",
        "                    total_links_calc = internal_links + external_links\n",
        "                    df.at[index, 'ratio_intHyperlinks'] = internal_links / total_links_calc if total_links_calc > 0 else 0.0\n",
        "                    df.at[index, 'ratio_extHyperlinks'] = external_links / total_links_calc if total_links_calc > 0 else 0.0\n",
        "\n",
        "\n",
        "                    # --- 6. ratio_extRedirection (å¤–éƒ¨é‡æ–°å°å‘çš„æ¯”ä¾‹) ---\n",
        "                    # æª¢æŸ¥å¤–éƒ¨é€£çµæ˜¯å¦åŒ…å«é‡å®šå‘ç›¸é—œçš„å±¬æ€§æˆ–JavaScript\n",
        "                    redirect_count = 0\n",
        "                    for link_tag in all_links:\n",
        "                        href = link_tag['href']\n",
        "                        if href.startswith('#'):\n",
        "                            continue\n",
        "                        full_url = urljoin(url, href)\n",
        "                        linked_parsed = urlparse(full_url)\n",
        "                        linked_domain = linked_parsed.netloc\n",
        "                        is_external = (linked_domain != parsed_url.netloc)\n",
        "\n",
        "                        if is_external:\n",
        "                            # æª¢æŸ¥æ˜¯å¦æœ‰é‡å®šå‘ç›¸é—œçš„å±¬æ€§\n",
        "                            if link_tag.get('onclick') and 'window.location' in link_tag.get('onclick', ''):\n",
        "                                redirect_count += 1\n",
        "                            elif link_tag.get('target') == '_blank' and 'redirect' in link_tag.get_text().lower():\n",
        "                                redirect_count += 1\n",
        "\n",
        "                    df.at[index, 'ratio_extRedirection'] = redirect_count / len(all_links) if all_links else 0.0\n",
        "\n",
        "                    # --- 7. ratio_extErrors (å¤–éƒ¨é€£çµä¸­è¿”å›éŒ¯èª¤çš„æ¯”ä¾‹) ---\n",
        "                    # æª¢æŸ¥å¤–éƒ¨é€£çµæ˜¯å¦æŒ‡å‘æ˜é¡¯éŒ¯èª¤çš„URLæ ¼å¼\n",
        "                    error_count = 0\n",
        "                    for link_tag in all_links:\n",
        "                        href = link_tag['href']\n",
        "                        if href.startswith('#'):\n",
        "                            continue\n",
        "                        full_url = urljoin(url, href)\n",
        "                        linked_parsed = urlparse(full_url)\n",
        "                        linked_domain = linked_parsed.netloc\n",
        "                        is_external = (linked_domain != parsed_url.netloc)\n",
        "\n",
        "                        if is_external:\n",
        "                            # æª¢æŸ¥æ˜¯å¦ç‚ºæ˜é¡¯éŒ¯èª¤çš„URL\n",
        "                            if 'error' in full_url.lower() or '404' in full_url or 'notfound' in full_url.lower():\n",
        "                                error_count += 1\n",
        "                            elif not linked_domain or linked_domain == '':\n",
        "                                error_count += 1\n",
        "\n",
        "                    df.at[index, 'ratio_extErrors'] = error_count / len(all_links) if all_links else 0.0\n",
        "\n",
        "                    # --- 8. external_favicon: ç¶²ç«™æ˜¯å¦ä½¿ç”¨ä¾†è‡ªå¤–éƒ¨åŸŸåçš„ Favicon ---\n",
        "                    favicon_link = soup_dynamic.find('link', rel=lambda x: x and 'icon' in x.lower())\n",
        "                    df.at[index, 'external_favicon'] = 0.0\n",
        "                    if favicon_link and 'href' in favicon_link.attrs:\n",
        "                        favicon_url = urljoin(url, favicon_link['href'])\n",
        "                        favicon_domain = urlparse(favicon_url).netloc\n",
        "                        if favicon_domain != parsed_url.netloc:\n",
        "                            df.at[index, 'external_favicon'] = 1.0\n",
        "\n",
        "                    # --- 9. links_in_tags: ç‰¹å®š HTML æ¨™ç±¤ï¼ˆå¦‚ <a>ã€<script>ï¼‰ä¸­é€£çµçš„æ•¸é‡ ---\n",
        "                    # é€™è£¡çµ±è¨ˆæ‰€æœ‰ href å’Œ src å±¬æ€§çš„é€£çµ\n",
        "                    total_links_in_tags = 0\n",
        "                    for tag in soup_dynamic.find_all(['a', 'script', 'img', 'link', 'iframe', 'form']):\n",
        "                        if 'href' in tag.attrs:\n",
        "                            total_links_in_tags += 1\n",
        "                        if 'src' in tag.attrs:\n",
        "                            total_links_in_tags += 1\n",
        "                        if tag.name == 'form' and 'action' in tag.attrs:\n",
        "                            total_links_in_tags += 1\n",
        "                    df.at[index, 'links_in_tags'] = total_links_in_tags\n",
        "\n",
        "                    # --- 10. ratio_extMedia: å¤–éƒ¨åª’é«”ï¼ˆåœ–ç‰‡ã€éŸ³é »ã€è¦–é »ï¼‰çš„æ¯”ä¾‹ ---\n",
        "                    media_tags = soup_dynamic.find_all(['img', 'audio', 'video', 'source'])\n",
        "                    total_media = len(media_tags)\n",
        "                    external_media = 0\n",
        "                    for media_tag in media_tags:\n",
        "                        src = media_tag.get('src') or media_tag.get('href')\n",
        "                        if src:\n",
        "                            media_url = urljoin(url, src)\n",
        "                            media_domain = urlparse(media_url).netloc\n",
        "                            if media_domain != parsed_url.netloc:\n",
        "                                external_media += 1\n",
        "                    df.at[index, 'ratio_extMedia'] = external_media / total_media if total_media > 0 else 0.0\n",
        "\n",
        "                    # --- 11. safe_anchor: éŒ¨é»é€£çµæ˜¯å¦å®‰å…¨ï¼ˆä¾‹å¦‚é¿å…æŒ‡å‘å¯ç–‘å¤–éƒ¨ç¶²ç«™ï¼‰ ---\n",
        "                    # æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å‘ IP åœ°å€ã€ä¸å¸¸è¦‹å”è­°æˆ–å¯ç–‘åŸŸåçš„å¤–éƒ¨é€£çµ\n",
        "                    df.at[index, 'safe_anchor'] = 1.0\n",
        "                    suspicious_keywords = ['bit.ly', 'tinyurl', 'goo.gl', 't.co', 'fb.me', 'is.gd']\n",
        "\n",
        "                    for link_tag in all_links:\n",
        "                        href = link_tag['href']\n",
        "                        if href.startswith('#'):\n",
        "                            continue\n",
        "                        full_url = urljoin(url, href)\n",
        "                        linked_parsed = urlparse(full_url)\n",
        "                        linked_domain = linked_parsed.netloc\n",
        "                        is_external = (linked_domain != parsed_url.netloc)\n",
        "\n",
        "                        if is_external:\n",
        "                            # æª¢æŸ¥æ˜¯å¦ç‚ºIPåœ°å€\n",
        "                            try:\n",
        "                                ipaddress.ip_address(linked_domain)\n",
        "                                df.at[index, 'safe_anchor'] = 0.0\n",
        "                                break\n",
        "                            except ValueError:\n",
        "                                pass\n",
        "\n",
        "                            # æª¢æŸ¥å”è­°æ˜¯å¦å®‰å…¨\n",
        "                            if linked_parsed.scheme not in ['http', 'https', '']:\n",
        "                                df.at[index, 'safe_anchor'] = 0.0\n",
        "                                break\n",
        "\n",
        "                            # æª¢æŸ¥æ˜¯å¦ç‚ºå¯ç–‘çš„çŸ­ç¶²å€æœå‹™\n",
        "                            if any(keyword in linked_domain.lower() for keyword in suspicious_keywords):\n",
        "                                df.at[index, 'safe_anchor'] = 0.0\n",
        "                                break\n",
        "\n",
        "                    # --- 12. empty_title: ç¶²é æ¨™é¡Œæ˜¯å¦ç‚ºç©º ---\n",
        "                    df.at[index, 'empty_title'] = 1.0 if not (soup_dynamic.title and soup_dynamic.title.string and soup_dynamic.title.string.strip()) else 0.0\n",
        "\n",
        "                    # --- 13. domain_in_title: åŸŸåæ˜¯å¦å‡ºç¾åœ¨ç¶²é æ¨™é¡Œä¸­ ---\n",
        "                    df.at[index, 'domain_in_title'] = 0.0\n",
        "                    if soup_dynamic.title and soup_dynamic.title.string:\n",
        "                        if base_domain in soup_dynamic.title.string.lower():\n",
        "                            df.at[index, 'domain_in_title'] = 1.0\n",
        "\n",
        "                    # --- 14. domain_with_copyright: ç¶²ç«™çš„ç‰ˆæ¬Šè³‡è¨Šä¸­æ˜¯å¦åŒ…å«åŸŸå ---\n",
        "                    df.at[index, 'domain_with_copyright'] = 0.0\n",
        "\n",
        "                    # æª¢æŸ¥ç‰ˆæ¬Šæ–‡æœ¬\n",
        "                    copyright_text = soup_dynamic.find(text=re.compile(r'Â©|copyright', re.IGNORECASE))\n",
        "                    if copyright_text and base_domain in copyright_text.lower():\n",
        "                        df.at[index, 'domain_with_copyright'] = 1.0\n",
        "\n",
        "                    # æª¢æŸ¥footerå€åŸŸ\n",
        "                    footer_tags = soup_dynamic.find_all(['div', 'footer'], class_=re.compile(r'footer|copyright', re.IGNORECASE))\n",
        "                    for footer in footer_tags:\n",
        "                        if base_domain in footer.get_text().lower():\n",
        "                            df.at[index, 'domain_with_copyright'] = 1.0\n",
        "                            break\n",
        "\n",
        "                    # æª¢æŸ¥æ‰€æœ‰åŒ…å«ç‰ˆæ¬Šç›¸é—œæ–‡å­—çš„æ¨™ç±¤\n",
        "                    copyright_tags = soup_dynamic.find_all(text=re.compile(r'Â©|copyright|all rights reserved', re.IGNORECASE))\n",
        "                    for tag in copyright_tags:\n",
        "                        if base_domain in tag.lower():\n",
        "                            df.at[index, 'domain_with_copyright'] = 1.0\n",
        "                            break\n",
        "\n",
        "                    df.at[index, 'feature_extracted'] = 1.0\n",
        "                    print('âœ… è³‡æ–™è™•ç†å®Œæˆ')\n",
        "                except Exception as e:\n",
        "                    print(f\"è§£æ URL '{url}' çš„ HTML æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "                    for col in html_feature_columns:\n",
        "                        df.at[index, col] = 0.0\n",
        "            else:\n",
        "                print(f\"æœªèƒ½ç²å– URL '{url}' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚\")\n",
        "            time.sleep(0.5)\n",
        "            SAVE_INTERVAL = 300\n",
        "            if (index + 1) % SAVE_INTERVAL == 0:\n",
        "                print(f\"--- å·²è™•ç† {index+1} ç­†ï¼Œæ­£åœ¨å„²å­˜é€²åº¦... ---\")\n",
        "                df.to_csv('phishing_dataset_expansion_forEmbeddingModule_html.csv', index=False)\n",
        "        driver.quit()\n",
        "        return df\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        driver.quit()\n",
        "        # df.to_csv('phishing_dataset_expansion_forEmbeddingModule_html.csv', index=False)\n",
        "        print(\"\\nğŸ›‘ KeyboardInterrupt (Ctrl+C) received by user.\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "import signal\n",
        "\n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def handler(signum, frame):\n",
        "    raise TimeoutException()\n",
        "\n",
        "def safe_process_url(driver, url, max_time=30):\n",
        "    signal.signal(signal.SIGALRM, handler)\n",
        "    signal.alarm(max_time)  # è¨­å®šæœ€å¤šè™•ç† max_time ç§’\n",
        "    try:\n",
        "        html_content, has_js_redirect = get_html_content(url, timeout=20, max_retries=1)\n",
        "        dynamic_content = None\n",
        "        if html_content:\n",
        "            dynamic_content, state = _fetch_dynamic_content(driver, url)\n",
        "\n",
        "        # æˆåŠŸå°±æ¸…æ‰ alarm\n",
        "        signal.alarm(0)\n",
        "        return html_content, dynamic_content\n",
        "    except TimeoutException:\n",
        "        print(f\"â° URL {url} è¶…é {max_time} ç§’ï¼Œå¼·åˆ¶è·³é\")\n",
        "        return None, False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ URL {url} è™•ç†å¤±æ•—: {e}\")\n",
        "        return None, False\n",
        "\n",
        "\n",
        "# --- ç¯„ä¾‹ä½¿ç”¨ ---\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists('phishing_dataset_expansion_forEmbeddingModule_html.csv'):\n",
        "\n",
        "        print(f\"æ‰¾åˆ°é€²åº¦æª”: {'phishing_dataset_expansion_forEmbeddingModule_html.csv'}ã€‚æ­£åœ¨è¼‰å…¥ä¸¦ç¹¼çºŒä»»å‹™...\")\n",
        "        try:\n",
        "            df= pd.read_csv('phishing_dataset_expansion_forEmbeddingModule_html.csv')\n",
        "        except Exception as e:\n",
        "            print(f\"è®€å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚\")\n",
        "            exit()\n",
        "    else:\n",
        "        print('error')\n",
        "        exit()\n",
        "    # å‘¼å«å‡½å¼å‰µå»ºæ–°ç‰¹å¾µ\n",
        "    # æ³¨æ„: æœƒè‡ªå‹•å…ˆä½¿ç”¨ requestsï¼Œå¦‚æœå…§å®¹ç‚ºç©ºå‰‡ä½¿ç”¨ Selenium\n",
        "    df = extract_html_features(df, url_column='url')\n",
        "\n",
        "    # save_df(df_with_html_features, 'output_with_features_5000toEnd.csv')\n",
        "    df.to_csv('phishing_dataset_expansion_forEmbeddingModule_html.csv', index=False)\n",
        "    print('å­˜æª”æˆåŠŸ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "id": "W_Sg42O7GLXj",
        "outputId": "511d60cd-e45b-4d97-c308-f8ab4ddc1c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ‰¾åˆ°é€²åº¦æª”: phishing_dataset_expansion_forEmbeddingModule_html.csvã€‚æ­£åœ¨è¼‰å…¥ä¸¦ç¹¼çºŒä»»å‹™...\n",
            "é–‹å§‹è¨­å®šDRIVER...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"6fb40be8-f15f-49ae-b605-f44514a8461f-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"6fb40be8-f15f-49ae-b605-f44514a8461f-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"6fb40be8-f15f-49ae-b605-f44514a8461f-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"6fb40be8-f15f-49ae-b605-f44514a8461f-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WebDriver åˆå§‹åŒ–å®Œæˆã€‚\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2078 / 100000 ç­† URL: http://www.antongas-fx.ru\n",
            "[é‡è©¦ 1/1] Requests ç²å– http://www.antongas-fx.ru æ™‚ç™¼ç”ŸéŒ¯èª¤: HTTPConnectionPool(host='www.antongas-fx.ru', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7e383b49f470>: Failed to resolve 'www.antongas-fx.ru' ([Errno -2] Name or service not known)\"))\n",
            "æœªèƒ½ç²å– URL 'http://www.antongas-fx.ru' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2079 / 100000 ç­† URL: https://securebusiness.github.io/bt-1/load.html\n",
            "âš ï¸  requests ç²å–çš„å…§å®¹ç‚ºç©ºæˆ–éçŸ­ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium\n",
            "æœªèƒ½ç²å– URL 'https://securebusiness.github.io/bt-1/load.html' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'securebusiness.github.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨è™•ç†ç¬¬ 2080 / 100000 ç­† URL: https://www.youlikehits.com\n",
            "âœ… ä½¿ç”¨ requests æˆåŠŸç²å– https://www.youlikehits.com çš„å…§å®¹\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.youlikehits.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ç¶²é  https://www.youlikehits.com å‹•æ…‹çˆ¬å–å®Œæˆ...\n",
            "âœ… è³‡æ–™è™•ç†å®Œæˆ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-685878212.py:405: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  copyright_text = soup_dynamic.find(text=re.compile(r'Â©|copyright', re.IGNORECASE))\n",
            "/tmp/ipython-input-685878212.py:417: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  copyright_tags = soup_dynamic.find_all(text=re.compile(r'Â©|copyright|all rights reserved', re.IGNORECASE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨è™•ç†ç¬¬ 2081 / 100000 ç­† URL: https://newweb.wellpark.ac.nz/tmcb.nrn/pnci.php\n",
            "[é‡è©¦ 1/1] Requests ç²å– https://newweb.wellpark.ac.nz/tmcb.nrn/pnci.php æ™‚ç™¼ç”ŸéŒ¯èª¤: HTTPSConnectionPool(host='newweb.wellpark.ac.nz', port=443): Max retries exceeded with url: /tmcb.nrn/pnci.php (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e38398d4a10>: Failed to resolve 'newweb.wellpark.ac.nz' ([Errno -2] Name or service not known)\"))\n",
            "æœªèƒ½ç²å– URL 'https://newweb.wellpark.ac.nz/tmcb.nrn/pnci.php' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2082 / 100000 ç­† URL: http://www.tg0913.com\n",
            "[é‡è©¦ 1/1] Requests ç²å– http://www.tg0913.com æ™‚ç™¼ç”ŸéŒ¯èª¤: HTTPConnectionPool(host='www.tg0913.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7e38398d6a80>: Failed to resolve 'www.tg0913.com' ([Errno -2] Name or service not known)\"))\n",
            "æœªèƒ½ç²å– URL 'http://www.tg0913.com' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2083 / 100000 ç­† URL: https://www.ukrainedate.com\n",
            "âœ… ä½¿ç”¨ requests æˆåŠŸç²å– https://www.ukrainedate.com çš„å…§å®¹\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ukrainedate.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ç¶²é  https://www.ukrainedate.com å‹•æ…‹çˆ¬å–å®Œæˆ...\n",
            "âœ… è³‡æ–™è™•ç†å®Œæˆ\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2084 / 100000 ç­† URL: https://www.tv5monde.com\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.tv5monde.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ URL https://www.tv5monde.com è¿”å› HTTP 403 éŒ¯èª¤ï¼Œç›´æ¥è·³é\n",
            "æœªèƒ½ç²å– URL 'https://www.tv5monde.com' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2085 / 100000 ç­† URL: https://www.buyoutsinsider.com\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.buyoutsinsider.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ä½¿ç”¨ requests æˆåŠŸç²å– https://www.buyoutsinsider.com çš„å…§å®¹\n",
            "âœ… ç¶²é  https://www.buyoutsinsider.com å‹•æ…‹çˆ¬å–å®Œæˆ...\n",
            "âœ… è³‡æ–™è™•ç†å®Œæˆ\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2086 / 100000 ç­† URL: https://www.operating-system.org\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.operating-system.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ä½¿ç”¨ requests æˆåŠŸç²å– https://www.operating-system.org çš„å…§å®¹\n",
            "âœ… ç¶²é  https://www.operating-system.org å‹•æ…‹çˆ¬å–å®Œæˆ...\n",
            "âœ… è³‡æ–™è™•ç†å®Œæˆ\n",
            "æ­£åœ¨è™•ç†ç¬¬ 2087 / 100000 ç­† URL: https://www.historicalcharts.noaa.gov\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.historicalcharts.noaa.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ä½¿ç”¨ requests æˆåŠŸç²å– https://www.historicalcharts.noaa.gov çš„å…§å®¹\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfEkhZtYqFArT2M6/0x4he",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}